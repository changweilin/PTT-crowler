{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XLMfGvYgCdF6"
   },
   "outputs": [],
   "source": [
    "# Integrate and pick out suitable PTT contents for training RNN and seq2seq model.\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import types \n",
    "\n",
    "#Initialize for files name and path.\n",
    "base_dir = 'C:/Users/User/Raw data/PTT'\n",
    "totalboard_name = 'TJ-BG-AT-SX-CC-MV'\n",
    "board_name = ['Tech_job','Boy-Girl','AllTogether','sex','C_Chat','movie']\n",
    "board_dict = {'Tech_job':'1519748408.0598423','Boy-Girl':'1520323590.6503718',\n",
    "              'AllTogether':'1519135097.7481742','sex':'1520269282.2598605-merge',\n",
    "              'C_Chat':'1520385092.0256078-merge','movie':'1520492468.9048126-merge'}\n",
    "board_dict_index = {'Tech_job':0,'Boy-Girl':1,'AllTogether':2,'sex':3,'C_Chat':4,'movie':5}\n",
    "remove_type = ['無題', '轉錄', '轉載', '轉發', '轉貼', '轉潑', '轉PO', '公告', '判決', '申請', '申訴', '水桶', '教學', 'ANSI', '市調', '問卷', '廣告', '協尋', '尋人']\n",
    "max_word_length = 256\n",
    "min_word_length = 100\n",
    "max_title_length = 20\n",
    "min_title_length = 4\n",
    "label_newnum = len(board_name)\n",
    "\n",
    "board_folder = []\n",
    "board_dir = []\n",
    "content_dir = []\n",
    "push_dir = []\n",
    "new_content_dir = []\n",
    "totalboard_dir = os.path.join(base_dir, totalboard_name)\n",
    "if not os.path.exists(totalboard_dir):\n",
    "    os.makedirs(totalboard_dir)\n",
    "\n",
    "for name_ind in range(len(board_name)):\n",
    "    # Original data path.\n",
    "    board_folder.append(board_name[name_ind] + '_' + board_dict[board_name[name_ind]])\n",
    "    board_dir.append(os.path.join(base_dir, board_folder[name_ind]))\n",
    "    if not os.path.exists(board_dir[name_ind]):\n",
    "        os.makedirs(board_dir[name_ind])\n",
    "    content_dir.append(os.path.join(board_dir[name_ind], 'content'))\n",
    "    if not os.path.exists(content_dir[name_ind]):\n",
    "        os.makedirs(content_dir[name_ind])\n",
    "    push_dir.append(os.path.join(board_dir[name_ind], 'push'))\n",
    "    if not os.path.exists(push_dir[name_ind]):\n",
    "        os.makedirs(push_dir[name_ind])\n",
    "    # New data path\n",
    "    new_content_dir.append(os.path.join(totalboard_dir, board_name[name_ind]+'_content'))\n",
    "    if not os.path.exists(new_content_dir[name_ind]):\n",
    "        os.makedirs(new_content_dir[name_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {},
      {}
     ]
    },
    "colab_type": "code",
    "id": "MH-viBv5wX6f",
    "outputId": "edf51ad9-2f86-4f29-a23b-f7a00e114796",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\User\\Anaconda3\\Lib\\site-packages\\jieba\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.ufa6ae29b0cbce8b45e006c7fa30eaaf8.cache\n",
      "Loading model cost 0.979 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ..., board:  Tech_job\n",
      "This board was end execution, post number:  14717\n",
      "Running ..., board:  Boy-Girl\n",
      "This board was end execution, post number:  15256\n",
      "Running ..., board:  AllTogether\n",
      "This board was end execution, post number:  25040\n",
      "Running ..., board:  sex\n",
      "This board was end execution, post number:  15491\n",
      "Running ..., board:  C_Chat\n",
      "This board was end execution, post number:  81274\n",
      "Running ..., board:  movie\n",
      "This board was end execution, post number:  30915\n",
      "Board name:  ['Tech_job', 'Boy-Girl', 'AllTogether', 'sex', 'C_Chat', 'movie']\n",
      "Post number:  [14717, 15256, 25040, 15491, 81274, 30915]\n",
      "min :  [100, 100, 100, 100, 100, 100]\n",
      "q10 :  [109.0, 116.0, 107.0, 113.0, 110.0, 111.0]\n",
      "q25 :  [124.0, 139.0, 120.0, 133.0, 128.0, 127.0]\n",
      "q50 :  [157.0, 177.0, 150.0, 172.0, 163.0, 162.0]\n",
      "q75 :  [201.0, 216.0, 191.0, 213.0, 206.0, 205.0]\n",
      "q90 :  [233.0, 240.0, 225.10000000000218, 238.0, 235.0, 235.0]\n",
      "max :  [256, 256, 256, 256, 256, 256]\n",
      "mean :  [163.95501800638718, 177.67665180912428, 158.19644568690097, 173.66974372216126, 167.9610084405837, 167.54546336729743]\n",
      "std :  [45.285061159247704, 44.95121061240856, 43.452870449536064, 45.54497944103346, 45.45813712578686, 45.205921797594385]\n"
     ]
    }
   ],
   "source": [
    "#整理、修改各大看版原始數據\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import jieba\n",
    "\n",
    "# Import dictionary fron jieba and PTT.\n",
    "jieba.set_dictionary('C:/Users/User/Anaconda3/Lib/site-packages/jieba/dict.txt.big')\n",
    "jieba.load_userdict('C:/Users/User/Anaconda3/Lib/site-packages/jieba/userdict.txt')\n",
    "\n",
    "board_post_num = []\n",
    "board_word_num_tpye = ['min','q10','q25','q50','q75','q90','max','mean','std']\n",
    "board_word_num = {'min':[],'q10':[],'q25':[],'q50':[],'q75':[],'q90':[],'max':[],'mean':[],'std':[]}\n",
    "word_filters='`|＼｜＊／＝≦≧＿＠＃＄％︿＆～§◎．※↔│○●☆★◇◆□■▽▼△▲㊣⊙⊕ˍ…﹌﹋﹎﹍﹉﹊‥–↑↓←→↖↗↙↘∥∕℅≒≡∩∪∞￣＿◤◥◣◢∵∴〒⊥∠⊿┼┴┬┤├▔─│▕┌┐└┘╭╮╰╯═╞╪╡╔╦╗╠╬╣╚╩╝╒╤╕╘╧╛╓╥╖╟╫╢╙╨╜║▓╱╲╳▁▂▄▅▆▇█▏▎▍▌▋▊▉▁▔'\n",
    "\n",
    "def arrange_content(content_text):\n",
    "    # Remove http line in content.\n",
    "    http_local = content_text.rfind('http')\n",
    "    while http_local >= 0:\n",
    "        http_str = content_text.rfind('\\n',0 , http_local)\n",
    "        if http_str < 0:\n",
    "            http_str = 0\n",
    "        http_end = content_text.find('\\n', http_local)\n",
    "        if http_end < 0:\n",
    "            http_end = len(content_text)\n",
    "        content_text = content_text.replace(content_text[http_str:http_end], '')\n",
    "        http_local = content_text.rfind('http', 0, http_str)\n",
    "    # Remove filtered CHARs in content.\n",
    "    for char_filter in word_filters:\n",
    "        http_local = content_text.rfind(char_filter)\n",
    "        while http_local >= 0:\n",
    "            content_text = content_text.replace(char_filter, '')\n",
    "            http_local = content_text.rfind(char_filter, 0, http_local)\n",
    "    return content_text\n",
    "                        \n",
    "for name_ind in range(len(board_name)):\n",
    "    dfs = pd.read_csv(os.path.join(board_dir[name_ind], board_dict[board_name[name_ind]] + '.csv'))\n",
    "    board_post_num.append(len(dfs))\n",
    "    print(\"Running ..., board: \", board_name[name_ind])\n",
    "    for dfs_index in range(board_post_num[name_ind]-1, -1, -1):\n",
    "        read_index = int(dfs.iloc[dfs_index, 0])\n",
    "        type_name = str(dfs.iloc[dfs_index, 1])\n",
    "        word_length = int(dfs.iloc[dfs_index, 5])\n",
    "        title_name = str(dfs.iloc[dfs_index, 9])\n",
    "        title_length = len(title_name)\n",
    "        if not is_continue:\n",
    "            print(\"index: {:0>6d}\".format(dfs_index), end='\\r')\n",
    "        # Post type filter\n",
    "        is_continue = False\n",
    "        for remove in remove_type:\n",
    "            if type_name.find(remove)>=0:\n",
    "                is_continue = True\n",
    "                break\n",
    "        # Remove data if the type was not required.\n",
    "        if is_continue:\n",
    "            dfs.drop([dfs_index], inplace=True)\n",
    "            continue\n",
    "        # Remove data if title length was false.\n",
    "        if title_length > max_title_length or title_length < min_title_length:\n",
    "            dfs.drop([dfs_index], inplace=True)\n",
    "            continue\n",
    "        # Remove data if content word length was false.\n",
    "        if word_length > max_word_length*4 or word_length < min_word_length:\n",
    "            dfs.drop([dfs_index], inplace=True)\n",
    "            continue\n",
    "        # Read csv and remove file which was not existed.\n",
    "        text = ''\n",
    "        with open(os.path.join(content_dir[name_ind], str(read_index) + '.csv'), \n",
    "                    'r', encoding = 'utf-8-sig') as file:\n",
    "            csvCursor = csv.reader(file)\n",
    "            for rows in csvCursor:\n",
    "                for row in rows:\n",
    "                    # Read content and remove empty.\n",
    "                    row = arrange_content(row)\n",
    "                    text = text + row\n",
    "        \n",
    "        # Word length filter by jieba words slit.\n",
    "        text.encode('utf-8-sig')\n",
    "        text_jieba = jieba.cut(text, cut_all=False)\n",
    "        text = ''\n",
    "        word_length = 0\n",
    "        for word in text_jieba:\n",
    "            word_length += 1\n",
    "            text = text + ' ' + word\n",
    "        # Remove data if content word length was false after jieba slitting.\n",
    "        if word_length > max_word_length or word_length < min_word_length:\n",
    "            dfs.drop([dfs_index], inplace=True)\n",
    "            continue\n",
    "        # Save posts after arrange content.\n",
    "        with open(os.path.join(new_content_dir[name_ind], str(read_index) + '.csv'), \n",
    "                  'w', encoding = 'utf-8-sig') as file:\n",
    "            csvCursor = csv.writer(file)\n",
    "            csvCursor.writerow([text])\n",
    "        dfs.iloc[dfs_index, 5] = word_length\n",
    "    \n",
    "    dfs.to_csv(os.path.join(totalboard_dir, board_name[name_ind] + '_fix.csv'), \n",
    "               encoding = 'utf-8-sig', index=False)\n",
    "    \n",
    "    board_post_num[name_ind] = (len(dfs))\n",
    "    board_word_num['min'].append(dfs.iloc[:, 5].min())\n",
    "    board_word_num['q10'].append(dfs.iloc[:, 5].quantile(0.1))\n",
    "    board_word_num['q25'].append(dfs.iloc[:, 5].quantile(0.25))\n",
    "    board_word_num['q50'].append(dfs.iloc[:, 5].quantile(0.5))\n",
    "    board_word_num['q75'].append(dfs.iloc[:, 5].quantile(0.75))\n",
    "    board_word_num['q90'].append(dfs.iloc[:, 5].quantile(0.9))\n",
    "    board_word_num['max'].append(dfs.iloc[:, 5].max())\n",
    "    board_word_num['mean'].append(dfs.iloc[:, 5].mean())\n",
    "    board_word_num['std'].append(dfs.iloc[:, 5].std())\n",
    "    \n",
    "    print(\"This board was end execution, post number: \", board_post_num[name_ind])\n",
    "\n",
    "print('Board name: ', board_name)\n",
    "print('Post number: ', board_post_num)\n",
    "for qtype in board_word_num_tpye:\n",
    "    print(qtype, ': ', board_word_num[qtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#整合各大看板資料，挑選訓練、測試用數據\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "board_number_limit = 14400\n",
    "board_post_num = []\n",
    "\n",
    "# Select content each board by number=board_number_limit.\n",
    "for name_ind in range(len(board_name)):\n",
    "    dfs = pd.read_csv(os.path.join(totalboard_dir, board_name[name_ind] + '_fix.csv'))\n",
    "    board_post_num.append(len(dfs))\n",
    "    random_board = list(range(board_post_num[name_ind]))\n",
    "    random.shuffle(random_board)\n",
    "    for dfs_index in range(board_post_num[name_ind]):\n",
    "        if random_board[dfs_index] >= board_number_limit:\n",
    "            dfs.drop([dfs_index], inplace=True)\n",
    "    dfs.to_csv(os.path.join(totalboard_dir, board_name[name_ind] + '_select.csv'), \n",
    "               encoding = 'utf-8-sig', index=False)\n",
    "\n",
    "# Merge data each board\n",
    "for name_ind in range(len(board_name)):\n",
    "    dfs = pd.read_csv(os.path.join(totalboard_dir, board_name[name_ind] + '_select.csv'))\n",
    "    dfs.loc[:,'board_name'] = pd.Series(np.full(board_post_num[name_ind], board_name[name_ind]))\n",
    "    if name_ind > 0:\n",
    "        dfs_total = pd.concat([dfs_total, dfs])\n",
    "    else:\n",
    "        dfs_total = dfs\n",
    "\n",
    "# Upset and save merged data\n",
    "dfs_total.reset_index(drop=True ,inplace=True)\n",
    "dfs_total = dfs_total.sample(frac= 1.0 ,replace=True)\n",
    "dfs_total.to_csv(os.path.join(totalboard_dir, totalboard_name + '.csv'), \n",
    "                 encoding = 'utf-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "multi-PTT text classification model.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
