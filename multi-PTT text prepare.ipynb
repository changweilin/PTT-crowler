{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XLMfGvYgCdF6"
   },
   "outputs": [],
   "source": [
    "# Integrate and pick out suitable PTT contents for training RNN and seq2seq model.\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import types \n",
    "\n",
    "#Initialize for files name and path.\n",
    "base_dir = 'C:/Users/User/Raw data/PTT'\n",
    "totalboard_name = 'BG-CC-MV-SX-TJ-WT'\n",
    "board_name = ['Boy-Girl','C_Chat','movie','sex','Tech_job','WomenTalk']\n",
    "board_dict = {'Boy-Girl':'1520323590.6503718','C_Chat':'1520385092.0256078-merge',\n",
    "              'movie':'1520492468.9048126-merge','sex':'1520269282.2598605-merge',\n",
    "              'Tech_job':'1519748408.0598423','WomenTalk':'1520349866.2532854'}\n",
    "board_dict_index = {'Tech_job':0,'Boy-Girl':1,'AllTogether':2,'sex':3,'C_Chat':4,'movie':5}\n",
    "remove_type = ['無題', '轉錄', '轉載', '轉發', '轉貼', '轉潑', '轉PO', '公告', '判決', '申請', '申訴', '水桶', '教學', 'ANSI', '市調', '問卷', '廣告', '協尋', '尋人']\n",
    "max_word_length = 250\n",
    "min_word_length = 100\n",
    "max_title_length = 20\n",
    "min_title_length = 4\n",
    "id_len = 6\n",
    "label_newnum = len(board_name)\n",
    "\n",
    "board_folder = []\n",
    "board_dir = []\n",
    "content_dir = []\n",
    "push_dir = []\n",
    "new_content_dir = []\n",
    "new_push_dir = []\n",
    "totalboard_dir = os.path.join(base_dir, totalboard_name)\n",
    "if not os.path.exists(totalboard_dir):\n",
    "    os.makedirs(totalboard_dir)\n",
    "\n",
    "for name_ind in range(len(board_name)):\n",
    "    # Original data path.\n",
    "    board_folder.append(board_name[name_ind] + '_' + board_dict[board_name[name_ind]])\n",
    "    board_dir.append(os.path.join(base_dir, board_folder[name_ind]))\n",
    "    if not os.path.exists(board_dir[name_ind]):\n",
    "        os.makedirs(board_dir[name_ind])\n",
    "    content_dir.append(os.path.join(board_dir[name_ind], 'content'))\n",
    "    if not os.path.exists(content_dir[name_ind]):\n",
    "        os.makedirs(content_dir[name_ind])\n",
    "    push_dir.append(os.path.join(board_dir[name_ind], 'push'))\n",
    "    if not os.path.exists(push_dir[name_ind]):\n",
    "        os.makedirs(push_dir[name_ind])\n",
    "    # New data path\n",
    "    new_content_dir.append(os.path.join(totalboard_dir, board_name[name_ind]+'_content'))\n",
    "    if not os.path.exists(new_content_dir[name_ind]):\n",
    "        os.makedirs(new_content_dir[name_ind])\n",
    "    new_push_dir.append(os.path.join(totalboard_dir, board_name[name_ind]+'_push'))\n",
    "    if not os.path.exists(new_push_dir[name_ind]):\n",
    "        os.makedirs(new_push_dir[name_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {},
      {}
     ]
    },
    "colab_type": "code",
    "id": "MH-viBv5wX6f",
    "outputId": "edf51ad9-2f86-4f29-a23b-f7a00e114796",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\User\\Anaconda3\\Lib\\site-packages\\jieba\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.ufa6ae29b0cbce8b45e006c7fa30eaaf8.cache\n",
      "Loading model cost 0.967 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ..., board: Boy-Girl\n",
      "Original post number: 75291\n",
      "This board was end execution, post number: 9172\n",
      "Running ..., board: C_Chat\n",
      "Original post number: 280564\n",
      "This board was end execution, post number: 68066\n",
      "Running ..., board: movie\n",
      "Original post number: 128246\n",
      "This board was end execution, post number: 26451\n",
      "Running ..., board: sex\n",
      "Original post number: 71154\n",
      "This board was end execution, post number: 11082\n",
      "Running ..., board: Tech_job\n",
      "Original post number: 58934\n",
      "This board was end execution, post number: 15327\n",
      "Running ..., board: WomenTalk\n",
      "Original post number: 128693\n",
      "This board was end execution, post number: 44379\n",
      "Board name: ['Boy-Girl', 'C_Chat', 'movie', 'sex', 'Tech_job', 'WomenTalk']\n",
      "Post number: [9172, 68066, 26451, 11082, 15327, 44379]\n",
      "min :  [100, 100, 100, 100, 100, 100]\n",
      "q10 :  [119.0, 113.0, 112.0, 113.0, 109.0, 114.0]\n",
      "q25 :  [145.0, 133.0, 131.0, 133.0, 125.0, 133.0]\n",
      "q50 :  [181.0, 167.0, 166.0, 168.0, 156.0, 165.0]\n",
      "q75 :  [216.0, 206.0, 204.0, 206.0, 197.0, 203.0]\n",
      "q90 :  [237.0, 232.0, 231.0, 232.0, 227.0, 230.0]\n",
      "max :  [250, 250, 250, 250, 250, 250]\n",
      "mean :  [179.49095071958132, 170.16030029677077, 168.74605875014177, 170.25311315646996, 162.5637763424023, 168.62489015074698]\n",
      "std :  [42.21326429662325, 43.15189185212415, 42.85779504908924, 42.9047494699834, 42.58346074902229, 42.01042712104982]\n"
     ]
    }
   ],
   "source": [
    "#整理、修改各大看版原始數據\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import jieba\n",
    "\n",
    "# Import dictionary fron jieba and PTT.\n",
    "jieba.set_dictionary('C:/Users/User/Anaconda3/Lib/site-packages/jieba/dict.txt.big')\n",
    "jieba.load_userdict('C:/Users/User/Anaconda3/Lib/site-packages/jieba/userdict.txt')\n",
    "\n",
    "board_post_num = []\n",
    "board_word_num_tpye = ['min','q10','q25','q50','q75','q90','max','mean','std']\n",
    "board_word_num = {'min':[],'q10':[],'q25':[],'q50':[],'q75':[],'q90':[],'max':[],'mean':[],'std':[]}\n",
    "\n",
    "charabc = 'abcdefghijklmnopqrstuvwxyz'\n",
    "char123 = '0123456789'\n",
    "charabc123_dict = {}\n",
    "for char in charabc:\n",
    "    charabc123_dict[ord(char)] = None\n",
    "for char in char123:\n",
    "    charabc123_dict[ord(char)] = None\n",
    "\n",
    "# 如果輸入字串推測像ID (長度大於等於6且由字母與數字組合)，則回傳True\n",
    "def is_personal_id(content_text):\n",
    "    return len(content_text)>=id_len and content_text.strip(charabc)!='' and content_text.strip(char123)!='' and content_text.translate(charabc123_dict)==''\n",
    "\n",
    "#filters_char='`|＼｜＊／＝≦≧＿＠＃＄％︿＆～§◎．※↔│○●☆★◇◆□■▽▼△▲㊣⊙⊕ˍ…﹌﹋﹎﹍﹉﹊‥–↑↓←→↖↗↙↘∥∕℅≒≡∩∪∞￣＿◤◥◣◢∵∴〒⊥∠⊿┼┴┬┤├▔─│▕┌┐└┘╭╮╰╯═╞╪╡╔╦╗╠╬╣╚╩╝╒╤╕╘╧╛╓╥╖╟╫╢╙╨╜║▓╱╲╳▁▂▄▅▆▇█▏▎▍▌▋▊▉▁▔'\n",
    "filters_char='ˊˋ#*×/\\\\^_`|＼﹨｜〝〞‵′＊／≦≧＿＃⌒‿．§◎˙※ㄧ↔│ꄊ✂☁☛✈♥○●☆★◇◆□■▽▼△▲㊣⊙⊕ˍ…﹌﹋﹎﹍﹉﹊‥–↑↓←→↖↗↙↘∥∕℅≒≡∩∪∞￣＿◤◥◣◢∵∴〒⊥∠⊿┼┴┬┤├▔─│▕┌┐└┘╭╮╰╯═╞╪╡╔╦╗╠╬╣╚╩╝╒╤╕╘╧╛╓╥╖╟╫╢╙╨╜║▓░▒╱╲╳▁━▶▁▂▃▄▅▆▇█▉▊▋▋▌▍▎▏▐▔■▁▔' \n",
    "filters_dict = {ord('Ａ'):'a', ord('ａ'):'a', ord('Ｂ'):'b', ord('ｂ'):'b', \n",
    "                ord('Ｃ'):'c', ord('ｃ'):'c', ord('Ｄ'):'d', ord('ｄ'):'d', \n",
    "                ord('Ｅ'):'e', ord('ｅ'):'e', ord('Ｆ'):'f', ord('ｆ'):'f', \n",
    "                ord('Ｇ'):'g', ord('ｇ'):'g', ord('Ｈ'):'h', ord('ｈ'):'h', \n",
    "                ord('Ｉ'):'i', ord('ｉ'):'i', ord('Ｊ'):'j', ord('ｊ'):'j', \n",
    "                ord('Ｋ'):'k', ord('ｋ'):'k', ord('Ｌ'):'l', ord('ｌ'):'l', \n",
    "                ord('Ｍ'):'m', ord('ｍ'):'m', ord('Ｎ'):'n', ord('ｎ'):'n', \n",
    "                ord('Ｏ'):'o', ord('ｏ'):'o', ord('Ｐ'):'p', ord('ｐ'):'p', \n",
    "                ord('Ｑ'):'q', ord('ｑ'):'q', ord('Ｒ'):'r', ord('ｒ'):'r', \n",
    "                ord('Ｓ'):'s', ord('ｓ'):'s', ord('Ｔ'):'t', ord('ｔ'):'t', \n",
    "                ord('Ｕ'):'u', ord('ｕ'):'u', ord('Ｖ'):'v', ord('ｖ'):'v', \n",
    "                ord('Ｗ'):'w', ord('ｗ'):'w', ord('Ｘ'):'x', ord('ｘ'):'x', \n",
    "                ord('Ｙ'):'y', ord('ｙ'):'y', ord('Ｚ'):'z', ord('ｚ'):'z',\n",
    "                ord('０'):'0', ord('１'):'1', ord('２'):'2', ord('３'):'3', \n",
    "                ord('４'):'4', ord('５'):'5', ord('６'):'6', ord('７'):'7', \n",
    "                ord('８'):'8', ord('９'):'9', ord('＋'):'+', ord('－'):'-', \n",
    "                ord('％'):'%', ord('＆'):'&', ord('＄'):'$', ord('＝'):'=', \n",
    "                ord('＠'):'@'}\n",
    "for char in filters_char:\n",
    "    filters_dict[ord(char)] = None\n",
    "\n",
    "split_char = ',.!?$'\n",
    "dot_char = '。\\n~～'\n",
    "comma_char = '，、 　\\t；;：:\"'\n",
    "upper_brackets_char = '<([{＜﹝「『（｛［【《〖﹙“‘︵︷︹︻︽︿﹁﹃'\n",
    "lower_brackets_char = '>)]}＞﹞」』）｝］】》〗﹚”’︶︸︺︼︾﹀﹂﹄'\n",
    "split_dict = {ord('！'):'!', ord('？'):'?'}\n",
    "for char in dot_char:\n",
    "    split_dict[ord(char)] = '.'\n",
    "for char in comma_char:\n",
    "    split_dict[ord(char)] = ','\n",
    "for char in upper_brackets_char:\n",
    "    split_dict[ord(char)] = ','\n",
    "for char in lower_brackets_char:\n",
    "    split_dict[ord(char)] = ','\n",
    "\n",
    "def del_char(chars, del_index):\n",
    "    if del_index+1>=len(chars):\n",
    "        chars = chars[:del_index]\n",
    "    elif del_index==0:\n",
    "        chars = chars[1:]\n",
    "    else:\n",
    "        chars = chars[:del_index]+chars[del_index+1:]\n",
    "    return chars\n",
    "\n",
    "def del_same_split(chars):\n",
    "    is_same = 0\n",
    "    for i in range(len(chars)-1,0,-1):\n",
    "        if chars[i] == chars[i-1]:\n",
    "            is_same+=1\n",
    "            if is_same>2:\n",
    "                chars = del_char(chars, i)\n",
    "            else:\n",
    "                for char in split_char:\n",
    "                    if chars[i]==char:\n",
    "                        chars = del_char(chars, i)\n",
    "                        break\n",
    "        else:\n",
    "            is_same=0\n",
    "    return chars\n",
    "\n",
    "# 刪除輸入字串中的網址和特殊符號，將全形數字字母轉換成半形\n",
    "def arrange_content(content_text):\n",
    "    # Remove http line in content.\n",
    "    http_local = content_text.rfind('http')\n",
    "    while http_local >= 0:\n",
    "        http_str = content_text.rfind('\\n',0 , http_local)\n",
    "        if http_str < 0:\n",
    "            http_str = 0\n",
    "        http_end = content_text.find('\\n', http_local)\n",
    "        if http_end < 0:\n",
    "            http_end = len(content_text)\n",
    "        content_text = content_text.replace(content_text[http_str:http_end], '')\n",
    "        http_local = content_text.rfind('http', 0, http_str)\n",
    "        \n",
    "    # Remove filtered CHARs in content.\n",
    "    content_text = content_text.translate(filters_dict)\n",
    "    content_text = content_text.translate(split_dict)\n",
    "    content_text = del_same_split(content_text)\n",
    "    \n",
    "    # Word length filter by jieba words slit.\n",
    "    text_jieba = jieba.cut(content_text, cut_all=False)\n",
    "    content_text = ''\n",
    "    word_length = 0\n",
    "    for word in text_jieba:\n",
    "        if is_personal_id(word):\n",
    "            word = 'nameid' \n",
    "        word_length += 1\n",
    "        content_text = content_text + ' ' + word\n",
    "    return content_text\n",
    "\n",
    "def arrange_push(push_input):\n",
    "    # Read push content and remove empty.\n",
    "    pushs = push_input.split('\\n')\n",
    "    push_type = []\n",
    "    push_id = []\n",
    "    push_content = []\n",
    "    push_time = []\n",
    "    for push in pushs:\n",
    "        push = push.split(',')\n",
    "        if len(push)>4:\n",
    "            try:\n",
    "                strptime = time.strptime(push[4],' %m/%d %H:%M')\n",
    "            except BaseException:\n",
    "                continue\n",
    "            if (len(push_id)==0 or push_id[-1]!=push[2]):\n",
    "                push_time.append(strptime)\n",
    "                if push[1].rfind('推')>=0:\n",
    "                    push_type.append(1)\n",
    "                elif push[1].rfind('噓')>=0:\n",
    "                    push_type.append(-1)\n",
    "                else:\n",
    "                    push_type.append(0)\n",
    "                push_id.append(push[2])\n",
    "                push_content.append(arrange_content(push[3].lower()))\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    push_time[-1] = strptime\n",
    "                except BaseException:\n",
    "                    continue\n",
    "                if push[1].rfind('推')>=0:\n",
    "                    push_type[-1] += 1\n",
    "                elif push[1].rfind('噓')>=0:\n",
    "                    push_type[-1 ]+= -1\n",
    "                push_content[-1]+arrange_content(push[3].lower())\n",
    "    return push_type, push_id, push_content, push_time\n",
    "\n",
    "\n",
    "for name_ind in range(len(board_name)):\n",
    "    dfs = pd.read_csv(os.path.join(board_dir[name_ind], board_dict[board_name[name_ind]] + '.csv'))\n",
    "    board_post_num.append(len(dfs))\n",
    "    print('Running ..., board:', board_name[name_ind])\n",
    "    print('Original post number:', board_post_num[name_ind])\n",
    "    for dfs_index in range(board_post_num[name_ind]-1, -1, -1):\n",
    "        read_index = int(dfs.iloc[dfs_index, 0])\n",
    "        type_name = str(dfs.iloc[dfs_index, 1])\n",
    "        word_length = int(dfs.iloc[dfs_index, 5])\n",
    "        push_length = int(dfs.iloc[dfs_index, 6])\n",
    "        title_name = str(dfs.iloc[dfs_index, 9])\n",
    "        title_length = len(title_name)\n",
    "        \n",
    "        # Remove data if there was no push.\n",
    "        if push_length<=1:\n",
    "            dfs.drop([dfs_index], inplace=True)\n",
    "            continue\n",
    "            \n",
    "        # Post type filter\n",
    "        is_continue = False\n",
    "        for remove in remove_type:\n",
    "            if type_name.find(remove)>=0:\n",
    "                is_continue = True\n",
    "                break\n",
    "                \n",
    "        # Remove data if the type was not required.\n",
    "        if is_continue:\n",
    "            dfs.drop([dfs_index], inplace=True)\n",
    "            continue\n",
    "            \n",
    "        # Remove data if title length was false.\n",
    "        '''if title_length > max_title_length or title_length < min_title_length:\n",
    "            dfs.drop([dfs_index], inplace=True)\n",
    "            continue'''\n",
    "        \n",
    "        # Remove data if content word length was false.\n",
    "        if word_length > max_word_length*4 or word_length < min_word_length:\n",
    "            dfs.drop([dfs_index], inplace=True)\n",
    "            continue\n",
    "        \n",
    "        # Read csv and remove file which was not existed.\n",
    "        text = ''\n",
    "        with open(os.path.join(content_dir[name_ind], str(read_index) + '.csv'), \n",
    "                    'r', encoding = 'utf-8-sig') as file:\n",
    "            csvCursor = csv.reader(file)\n",
    "            for rows in csvCursor:\n",
    "                for row in rows:\n",
    "                    # Read content and remove empty.\n",
    "                    row = arrange_content(row.lower())\n",
    "                    text = text + row\n",
    "        \n",
    "        # Remove data if content word length was false after jieba slitting.\n",
    "        if word_length > max_word_length or word_length < min_word_length:\n",
    "            dfs.drop([dfs_index], inplace=True)\n",
    "            continue\n",
    "        \n",
    "        # Read csv and arrange push content.\n",
    "        with open(os.path.join(push_dir[name_ind], str(read_index) + '.csv'),\n",
    "                    'r', encoding = 'utf-8-sig') as file:\n",
    "            csvCursor = csv.reader(file)\n",
    "            for rows in csvCursor:\n",
    "                for row in rows:\n",
    "                    push_type, push_id, push_content, push_time = arrange_push(row)\n",
    "            push_length = len(push_content)\n",
    "                    \n",
    "        # Save posts after arrange post content.\n",
    "        with open(os.path.join(new_content_dir[name_ind], str(read_index) + '.csv'), \n",
    "                  'w', encoding = 'utf-8-sig') as file:\n",
    "            csvCursor = csv.writer(file)\n",
    "            csvCursor.writerow([text])\n",
    "        dfs.iloc[dfs_index, 5] = word_length\n",
    "            \n",
    "        # Save posts after arrange push content.\n",
    "        with open(os.path.join(new_push_dir[name_ind], str(read_index) + '.csv'), \n",
    "                  'w', encoding = 'utf-8-sig') as file:\n",
    "            csvCursor = csv.writer(file)\n",
    "            csvCursor.writerow(push_content)\n",
    "        dfs.iloc[dfs_index, 6] = push_length\n",
    "        \n",
    "        if not is_continue and dfs_index % 100 == 0:\n",
    "            print(\"index: {:0>6d}\".format(dfs_index), end='\\r')\n",
    "    \n",
    "    dfs.to_csv(os.path.join(new_content_dir[name_ind], board_name[name_ind] + '_fix.csv'), \n",
    "               encoding = 'utf-8-sig', index=False)\n",
    "    \n",
    "    board_post_num[name_ind] = (len(dfs))\n",
    "    board_word_num['min'].append(dfs.iloc[:, 5].min())\n",
    "    board_word_num['q10'].append(dfs.iloc[:, 5].quantile(0.1))\n",
    "    board_word_num['q25'].append(dfs.iloc[:, 5].quantile(0.25))\n",
    "    board_word_num['q50'].append(dfs.iloc[:, 5].quantile(0.5))\n",
    "    board_word_num['q75'].append(dfs.iloc[:, 5].quantile(0.75))\n",
    "    board_word_num['q90'].append(dfs.iloc[:, 5].quantile(0.9))\n",
    "    board_word_num['max'].append(dfs.iloc[:, 5].max())\n",
    "    board_word_num['mean'].append(dfs.iloc[:, 5].mean())\n",
    "    board_word_num['std'].append(dfs.iloc[:, 5].std())\n",
    "    \n",
    "    print(\"This board was end execution, post number:\", board_post_num[name_ind])\n",
    "\n",
    "print('Board name:', board_name)\n",
    "print('Post number:', board_post_num)\n",
    "for qtype in board_word_num_tpye:\n",
    "    print(qtype, ': ', board_word_num[qtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#整合各大看板資料，挑選訓練、測試用數據\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "board_number_limit = [9088,21632,18688,11008,15232,20352] #[9088,67968,26368,11008,15232,44288]\n",
    "board_post_num = []\n",
    "\n",
    "# Select content each board by number=board_number_limit.\n",
    "for name_ind in range(len(board_name)):\n",
    "    dfs = pd.read_csv(os.path.join(new_content_dir[name_ind], board_name[name_ind] + '_fix.csv'))\n",
    "    board_post_num.append(len(dfs))\n",
    "    random_board = list(range(board_post_num[name_ind]))\n",
    "    random.shuffle(random_board)\n",
    "    for dfs_index in range(board_post_num[name_ind]):\n",
    "        if random_board[dfs_index] >= board_number_limit[name_ind]:\n",
    "            dfs.drop([dfs_index], inplace=True)\n",
    "    dfs.to_csv(os.path.join(new_content_dir[name_ind], board_name[name_ind] + '_select.csv'), \n",
    "               encoding = 'utf-8-sig', index=False)\n",
    "\n",
    "# Merge data each board\n",
    "for name_ind in range(len(board_name)):\n",
    "    dfs = pd.read_csv(os.path.join(new_content_dir[name_ind], board_name[name_ind] + '_select.csv'))\n",
    "    dfs.loc[:,'board_name'] = pd.Series(np.full(board_post_num[name_ind], board_name[name_ind]))\n",
    "    if name_ind > 0:\n",
    "        dfs_total = pd.concat([dfs_total, dfs])\n",
    "    else:\n",
    "        dfs_total = dfs\n",
    "\n",
    "# Upset and save merged data\n",
    "dfs_total.reset_index(drop=True ,inplace=True)\n",
    "dfs_total = dfs_total.sample(frac= 1.0 ,replace=True)\n",
    "dfs_total.to_csv(os.path.join(totalboard_dir, totalboard_name + '.csv'), \n",
    "                 encoding = 'utf-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "multi-PTT text classification model.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
