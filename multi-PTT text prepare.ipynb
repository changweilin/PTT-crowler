{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XLMfGvYgCdF6"
   },
   "outputs": [],
   "source": [
    "# Integrate and pick out suitable PTT contents for training RNN and seq2seq model.\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import types \n",
    "\n",
    "#Initialize for files name and path.\n",
    "base_dir = 'C:/Users/User/Raw data/PTT'\n",
    "totalboard_name = 'TJ-BG-AT-SX-CC-MV'\n",
    "board_name = ['Tech_job','Boy-Girl','AllTogether','sex','C_Chat','movie']\n",
    "board_dict = {'Tech_job':'1519748408.0598423','Boy-Girl':'1520323590.6503718',\n",
    "              'AllTogether':'1519135097.7481742','sex':'1520269282.2598605-merge',\n",
    "              'C_Chat':'1520385092.0256078-merge','movie':'1520492468.9048126-merge'}\n",
    "board_dict_index = {'Tech_job':0,'Boy-Girl':1,'AllTogether':2,'sex':3,'C_Chat':4,'movie':5}\n",
    "remove_type = ['無題', '轉錄', '轉載', '轉發', '轉貼', '轉潑', '轉PO', '公告', '判決', '申請', '申訴', '水桶', '教學', 'ANSI', '市調', '問卷', '廣告', '協尋', '尋人']\n",
    "max_word_length = 500\n",
    "min_word_length = 100\n",
    "max_title_length = 20\n",
    "min_title_length = 4\n",
    "id_len = 6\n",
    "label_newnum = len(board_name)\n",
    "\n",
    "board_folder = []\n",
    "board_dir = []\n",
    "content_dir = []\n",
    "push_dir = []\n",
    "new_content_dir = []\n",
    "totalboard_dir = os.path.join(base_dir, totalboard_name)\n",
    "if not os.path.exists(totalboard_dir):\n",
    "    os.makedirs(totalboard_dir)\n",
    "\n",
    "for name_ind in range(len(board_name)):\n",
    "    # Original data path.\n",
    "    board_folder.append(board_name[name_ind] + '_' + board_dict[board_name[name_ind]])\n",
    "    board_dir.append(os.path.join(base_dir, board_folder[name_ind]))\n",
    "    if not os.path.exists(board_dir[name_ind]):\n",
    "        os.makedirs(board_dir[name_ind])\n",
    "    content_dir.append(os.path.join(board_dir[name_ind], 'content'))\n",
    "    if not os.path.exists(content_dir[name_ind]):\n",
    "        os.makedirs(content_dir[name_ind])\n",
    "    push_dir.append(os.path.join(board_dir[name_ind], 'push'))\n",
    "    if not os.path.exists(push_dir[name_ind]):\n",
    "        os.makedirs(push_dir[name_ind])\n",
    "    # New data path\n",
    "    new_content_dir.append(os.path.join(totalboard_dir, board_name[name_ind]+'_content'))\n",
    "    if not os.path.exists(new_content_dir[name_ind]):\n",
    "        os.makedirs(new_content_dir[name_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {},
      {}
     ]
    },
    "colab_type": "code",
    "id": "MH-viBv5wX6f",
    "outputId": "edf51ad9-2f86-4f29-a23b-f7a00e114796",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "Building prefix dict from C:\\Users\\User\\Anaconda3\\Lib\\site-packages\\jieba\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.ufa6ae29b0cbce8b45e006c7fa30eaaf8.cache\n",
      "Loading model cost 0.989 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ..., board:  Tech_job\n",
      "This board was end execution, post number:  24764\n",
      "Running ..., board:  Boy-Girl\n",
      "This board was end execution, post number:  38575\n",
      "Running ..., board:  AllTogether\n",
      "This board was end execution, post number:  24936\n",
      "Running ..., board:  sex\n",
      "This board was end execution, post number:  33456\n",
      "Running ..., board:  C_Chat\n",
      "This board was end execution, post number:  115468\n",
      "Running ..., board:  movie\n",
      "This board was end execution, post number:  55375\n",
      "Board name:  ['Tech_job', 'Boy-Girl', 'AllTogether', 'sex', 'C_Chat', 'movie']\n",
      "Post number:  [24764, 38575, 24936, 33456, 115468, 55375]\n",
      "min :  [100, 100, 100, 100, 100, 100]\n",
      "q10 :  [118.0, 130.0, 110.0, 125.0, 115.0, 118.0]\n",
      "q25 :  [153.0, 177.0, 128.0, 165.0, 143.0, 153.0]\n",
      "q50 :  [230.0, 260.0, 170.0, 241.0, 200.0, 229.0]\n",
      "q75 :  [330.0, 362.0, 253.0, 340.0, 291.0, 330.0]\n",
      "q90 :  [416.0, 440.0, 351.0, 425.0, 386.0, 418.0]\n",
      "max :  [500, 500, 500, 500, 500, 500]\n",
      "mean :  [248.66435147795187, 273.1140635126377, 202.37981231953802, 258.07290172166427, 227.05553919700696, 249.25919638826184]\n",
      "std :  [109.19488467919884, 111.76388142916463, 94.54272240864385, 109.06992421943477, 101.89076307658347, 109.9994824236023]\n"
     ]
    }
   ],
   "source": [
    "#整理、修改各大看版原始數據\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import jieba\n",
    "\n",
    "# Import dictionary fron jieba and PTT.\n",
    "jieba.set_dictionary('C:/Users/User/Anaconda3/Lib/site-packages/jieba/dict.txt.big')\n",
    "jieba.load_userdict('C:/Users/User/Anaconda3/Lib/site-packages/jieba/userdict.txt')\n",
    "\n",
    "board_post_num = []\n",
    "board_word_num_tpye = ['min','q10','q25','q50','q75','q90','max','mean','std']\n",
    "board_word_num = {'min':[],'q10':[],'q25':[],'q50':[],'q75':[],'q90':[],'max':[],'mean':[],'std':[]}\n",
    "\n",
    "charabc = 'abcdefghijklmnopqrstuvwxyz'\n",
    "char123 = '0123456789'\n",
    "charabc123_dict = {}\n",
    "for char in charabc:\n",
    "    charabc123_dict[ord(char)] = None\n",
    "for char in char123:\n",
    "    charabc123_dict[ord(char)] = None\n",
    "\n",
    "# 如果輸入字串推測像ID (長度大於等於6且由字母與數字組合)，則回傳True\n",
    "def is_personal_id(content_text):\n",
    "    return len(content_text)>=id_len and content_text.strip(charabc)!='' and content_text.strip(char123)!='' and content_text.translate(charabc123_dict)==''\n",
    "\n",
    "#filters_char='`|＼｜＊／＝≦≧＿＠＃＄％︿＆～§◎．※↔│○●☆★◇◆□■▽▼△▲㊣⊙⊕ˍ…﹌﹋﹎﹍﹉﹊‥–↑↓←→↖↗↙↘∥∕℅≒≡∩∪∞￣＿◤◥◣◢∵∴〒⊥∠⊿┼┴┬┤├▔─│▕┌┐└┘╭╮╰╯═╞╪╡╔╦╗╠╬╣╚╩╝╒╤╕╘╧╛╓╥╖╟╫╢╙╨╜║▓╱╲╳▁▂▄▅▆▇█▏▎▍▌▋▊▉▁▔'\n",
    "filters_char='<>!\"#$%&()*+,×-./:;=?@[\\\\]^_`{|}~\\t\\n　，。！：；、？＜＞﹝﹞「」『』（）｛｝［］【】《》〖〗﹙﹚“”‘’＼﹨｜〝〞‵′＋－＊／＝≦≧＿＠＃＄％⌒‿︵︷︹︻︽︿﹁﹃︶︸︺︼︾﹀﹂﹄＆～§◎．˙※ㄧ↔│ꄊ✂☁☛✈♥○●☆★◇◆□■▽▼△▲㊣⊙⊕ˍ…﹌﹋﹎﹍﹉﹊‥–↑↓←→↖↗↙↘∥∕℅≒≡∩∪∞￣＿◤◥◣◢∵∴〒⊥∠⊿┼┴┬┤├▔─│▕┌┐└┘╭╮╰╯═╞╪╡╔╦╗╠╬╣╚╩╝╒╤╕╘╧╛╓╥╖╟╫╢╙╨╜║▓░▒╱╲╳▁━▶▁▂▃▄▅▆▇█▉▊▋▋▌▍▎▏▐▔■▁▔' \n",
    "filters_dict = {ord('Ａ'):'a', ord('ａ'):'a', ord('Ｂ'):'b', ord('ｂ'):'b', \n",
    "                ord('Ｃ'):'c', ord('ｃ'):'c', ord('Ｄ'):'d', ord('ｄ'):'d', \n",
    "                ord('Ｅ'):'e', ord('ｅ'):'e', ord('Ｆ'):'f', ord('ｆ'):'f', \n",
    "                ord('Ｇ'):'g', ord('ｇ'):'g', ord('Ｈ'):'h', ord('ｈ'):'h', \n",
    "                ord('Ｉ'):'i', ord('ｉ'):'i', ord('Ｊ'):'j', ord('ｊ'):'j', \n",
    "                ord('Ｋ'):'k', ord('ｋ'):'k', ord('Ｌ'):'l', ord('ｌ'):'l', \n",
    "                ord('Ｍ'):'m', ord('ｍ'):'m', ord('Ｎ'):'n', ord('ｎ'):'n', \n",
    "                ord('Ｏ'):'o', ord('ｏ'):'o', ord('Ｐ'):'p', ord('ｐ'):'p', \n",
    "                ord('Ｑ'):'q', ord('ｑ'):'q', ord('Ｒ'):'r', ord('ｒ'):'r', \n",
    "                ord('Ｓ'):'s', ord('ｓ'):'s', ord('Ｔ'):'t', ord('ｔ'):'t', \n",
    "                ord('Ｕ'):'u', ord('ｕ'):'u', ord('Ｖ'):'v', ord('ｖ'):'v', \n",
    "                ord('Ｗ'):'w', ord('ｗ'):'w', ord('Ｘ'):'x', ord('ｘ'):'x', \n",
    "                ord('Ｙ'):'y', ord('ｙ'):'y', ord('Ｚ'):'z', ord('ｚ'):'z',\n",
    "                ord('０'):'0', ord('１'):'1', ord('２'):'2', ord('３'):'3', \n",
    "                ord('４'):'4', ord('５'):'5', ord('６'):'6', ord('７'):'7', \n",
    "                ord('８'):'8', ord('９'):'9'}\n",
    "for char_filter in filters_char:\n",
    "    filters_dict[ord(char_filter)] = None\n",
    "\n",
    "# 刪除輸入字串中的網址和特殊符號，將全形數字字母轉換成半形\n",
    "def arrange_content(content_text):\n",
    "    # Remove http line in content.\n",
    "    http_local = content_text.rfind('http')\n",
    "    while http_local >= 0:\n",
    "        http_str = content_text.rfind('\\n',0 , http_local)\n",
    "        if http_str < 0:\n",
    "            http_str = 0\n",
    "        http_end = content_text.find('\\n', http_local)\n",
    "        if http_end < 0:\n",
    "            http_end = len(content_text)\n",
    "        content_text = content_text.replace(content_text[http_str:http_end], '')\n",
    "        http_local = content_text.rfind('http', 0, http_str)\n",
    "        \n",
    "    # Remove filtered CHARs in content.\n",
    "    content_text = content_text.translate(filters_dict)\n",
    "    return content_text\n",
    "\n",
    "for name_ind in range(len(board_name)):\n",
    "    dfs = pd.read_csv(os.path.join(board_dir[name_ind], board_dict[board_name[name_ind]] + '.csv'))\n",
    "    board_post_num.append(len(dfs))\n",
    "    print(\"Running ..., board: \", board_name[name_ind])\n",
    "    for dfs_index in range(board_post_num[name_ind]-1, -1, -1):\n",
    "        read_index = int(dfs.iloc[dfs_index, 0])\n",
    "        type_name = str(dfs.iloc[dfs_index, 1])\n",
    "        word_length = int(dfs.iloc[dfs_index, 5])\n",
    "        title_name = str(dfs.iloc[dfs_index, 9])\n",
    "        title_length = len(title_name)\n",
    "        # Post type filter\n",
    "        is_continue = False\n",
    "        for remove in remove_type:\n",
    "            if type_name.find(remove)>=0:\n",
    "                is_continue = True\n",
    "                break\n",
    "        # Remove data if the type was not required.\n",
    "        if is_continue:\n",
    "            dfs.drop([dfs_index], inplace=True)\n",
    "            continue\n",
    "        # Remove data if title length was false.\n",
    "        if title_length > max_title_length or title_length < min_title_length:\n",
    "            dfs.drop([dfs_index], inplace=True)\n",
    "            continue\n",
    "        # Remove data if content word length was false.\n",
    "        if word_length > max_word_length*4 or word_length < min_word_length:\n",
    "            dfs.drop([dfs_index], inplace=True)\n",
    "            continue\n",
    "        # Read csv and remove file which was not existed.\n",
    "        text = ''\n",
    "        with open(os.path.join(content_dir[name_ind], str(read_index) + '.csv'), \n",
    "                    'r', encoding = 'utf-8-sig') as file:\n",
    "            csvCursor = csv.reader(file)\n",
    "            for rows in csvCursor:\n",
    "                for row in rows:\n",
    "                    # Read content and remove empty.\n",
    "                    row = arrange_content(row.lower())\n",
    "                    text = text + row\n",
    "        \n",
    "        # Word length filter by jieba words slit.\n",
    "        text.encode('utf-8-sig')\n",
    "        text_jieba = jieba.cut(text, cut_all=False)\n",
    "        text = ''\n",
    "        word_length = 0\n",
    "        for word in text_jieba:\n",
    "            if is_personal_id(word):\n",
    "                word = 'nameid' \n",
    "            word_length += 1\n",
    "            text = text + ' ' + word\n",
    "        # Remove data if content word length was false after jieba slitting.\n",
    "        if word_length > max_word_length or word_length < min_word_length:\n",
    "            dfs.drop([dfs_index], inplace=True)\n",
    "            continue\n",
    "        # Save posts after arrange content.\n",
    "        with open(os.path.join(new_content_dir[name_ind], str(read_index) + '.csv'), \n",
    "                  'w', encoding = 'utf-8-sig') as file:\n",
    "            csvCursor = csv.writer(file)\n",
    "            csvCursor.writerow([text])\n",
    "        dfs.iloc[dfs_index, 5] = word_length\n",
    "        if not is_continue and dfs_index % 1000 == 0:\n",
    "            print(\"index: {:0>6d}\".format(dfs_index), end='\\r')\n",
    "    \n",
    "    dfs.to_csv(os.path.join(new_content_dir[name_ind], board_name[name_ind] + '_fix.csv'), \n",
    "               encoding = 'utf-8-sig', index=False)\n",
    "    \n",
    "    board_post_num[name_ind] = (len(dfs))\n",
    "    board_word_num['min'].append(dfs.iloc[:, 5].min())\n",
    "    board_word_num['q10'].append(dfs.iloc[:, 5].quantile(0.1))\n",
    "    board_word_num['q25'].append(dfs.iloc[:, 5].quantile(0.25))\n",
    "    board_word_num['q50'].append(dfs.iloc[:, 5].quantile(0.5))\n",
    "    board_word_num['q75'].append(dfs.iloc[:, 5].quantile(0.75))\n",
    "    board_word_num['q90'].append(dfs.iloc[:, 5].quantile(0.9))\n",
    "    board_word_num['max'].append(dfs.iloc[:, 5].max())\n",
    "    board_word_num['mean'].append(dfs.iloc[:, 5].mean())\n",
    "    board_word_num['std'].append(dfs.iloc[:, 5].std())\n",
    "    \n",
    "    print(\"This board was end execution, post number: \", board_post_num[name_ind])\n",
    "\n",
    "print('Board name: ', board_name)\n",
    "print('Post number: ', board_post_num)\n",
    "for qtype in board_word_num_tpye:\n",
    "    print(qtype, ': ', board_word_num[qtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#整合各大看板資料，挑選訓練、測試用數據\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "board_number_limit = 24000\n",
    "board_post_num = []\n",
    "\n",
    "# Select content each board by number=board_number_limit.\n",
    "for name_ind in range(len(board_name)):\n",
    "    dfs = pd.read_csv(os.path.join(new_content_dir[name_ind], board_name[name_ind] + '_fix.csv'))\n",
    "    board_post_num.append(len(dfs))\n",
    "    random_board = list(range(board_post_num[name_ind]))\n",
    "    random.shuffle(random_board)\n",
    "    for dfs_index in range(board_post_num[name_ind]):\n",
    "        if random_board[dfs_index] >= board_number_limit:\n",
    "            dfs.drop([dfs_index], inplace=True)\n",
    "    dfs.to_csv(os.path.join(new_content_dir[name_ind], board_name[name_ind] + '_select.csv'), \n",
    "               encoding = 'utf-8-sig', index=False)\n",
    "\n",
    "# Merge data each board\n",
    "for name_ind in range(len(board_name)):\n",
    "    dfs = pd.read_csv(os.path.join(new_content_dir[name_ind], board_name[name_ind] + '_select.csv'))\n",
    "    dfs.loc[:,'board_name'] = pd.Series(np.full(board_post_num[name_ind], board_name[name_ind]))\n",
    "    if name_ind > 0:\n",
    "        dfs_total = pd.concat([dfs_total, dfs])\n",
    "    else:\n",
    "        dfs_total = dfs\n",
    "\n",
    "# Upset and save merged data\n",
    "dfs_total.reset_index(drop=True ,inplace=True)\n",
    "dfs_total = dfs_total.sample(frac= 1.0 ,replace=True)\n",
    "dfs_total.to_csv(os.path.join(totalboard_dir, totalboard_name + '.csv'), \n",
    "                 encoding = 'utf-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "multi-PTT text classification model.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
