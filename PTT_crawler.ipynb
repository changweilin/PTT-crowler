{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MZNGgVooINmw"
   },
   "outputs": [],
   "source": [
    "# Crawl content, title, push and web for all of PTT board.\n",
    "import time\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import sys\n",
    "import urllib3\n",
    "# Disable the checking of certification.\n",
    "urllib3.disable_warnings(False)\n",
    "\n",
    "page_index = 2000 #開始頁面\n",
    "end_index = 6463 + 1 #結束葉面\n",
    "start_post = 0 #貼文起始index (-5公告)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "output_extras": [
      {
       "item_id": 5
      }
     ]
    },
    "colab_type": "code",
    "id": "ON1ey6ZyINm4",
    "outputId": "e6a28d84-8069-4b19-e2d1-38e022e2a071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520514079.5375044\n",
      "page 6463\r"
     ]
    }
   ],
   "source": [
    "err_max = 10\n",
    "now_folder_len = 2000\n",
    "now_folder_cnt = 0\n",
    "\n",
    "start_index = page_index\n",
    "#end_index = 3697\n",
    "ptt_web = 'https://www.ptt.cc'\n",
    "board = 'movie'\n",
    "board_web_tail = '/bbs/' + board + '/index'\n",
    "board_web = ptt_web + board_web_tail\n",
    "user_name = 'user'\n",
    "base_dir = 'C:/Users/'+user_name+'/Raw data/PTT/'\n",
    "#base_dir = './drive/Raw data/PTT/'\n",
    "over18_web = ptt_web + '/ask/over18'\n",
    "payload18 = {\n",
    "    'from':board_web_tail +'.html',\n",
    "    'yes':'yes'\n",
    "}\n",
    "\n",
    "# Build folder for PTT content.\n",
    "now_time = time.time()\n",
    "print(now_time)\n",
    "nowtime_dir = os.path.join(base_dir, board+'_'+str(now_time))\n",
    "if not os.path.exists(nowtime_dir):\n",
    "    os.makedirs(nowtime_dir)\n",
    "content_dir = os.path.join(nowtime_dir, 'content')\n",
    "if not os.path.exists(content_dir):\n",
    "    os.makedirs(content_dir)\n",
    "push_dir = os.path.join(nowtime_dir, 'push')\n",
    "if not os.path.exists(push_dir):\n",
    "    os.makedirs(push_dir)\n",
    "csv_dir = nowtime_dir+'/'+str(now_time)+'.csv'\n",
    "\n",
    "def row_1st_keep(input_text):\n",
    "    row_1st_end = input_text.find('\\n')\n",
    "    if row_1st_end < 0:\n",
    "        return input_text[0:-1]\n",
    "    else:\n",
    "        return input_text[0:row_1st_end]\n",
    "\n",
    "po_ip = ''\n",
    "f_list = open(csv_dir ,'a', encoding = 'utf-8-sig')\n",
    "w_list = csv.writer(f_list)\n",
    "postInfo = ['post_index', 'type', 'week', 'time', 'id', 'word_num', 'push_num', 'web', 'ip', 'title']\n",
    "w_list.writerow(postInfo)\n",
    "f_list.close()\n",
    "for page_index in range(start_index,end_index):\n",
    "    #### Link web of PTT board and read posts web each page. ####\n",
    "    if (page_index - start_index) % now_folder_len == (now_folder_len - 1):\n",
    "        now_folder_cnt = now_folder_cnt + 1\n",
    "        csv_dir = nowtime_dir+'/'+str(now_time)+'_'+str(now_folder_cnt)+'.csv'\n",
    "    is_err = True\n",
    "    err_count = 0\n",
    "    rs = requests.session()\n",
    "    while is_err:\n",
    "        try:\n",
    "            res = rs.post(over18_web, data=payload18)\n",
    "            res = rs.get(ptt_web + board_web_tail + str(page_index) +'.html', verify=False)\n",
    "            #res = requests.get(board_web + str(page_index) +'.html', verify=False)\n",
    "        except BaseException:\n",
    "            err_count = err_count + 1\n",
    "            if err_count > err_max:\n",
    "                print('ERROR page: ', page_index, 'WEB: ', ptt_web + board_web_tail + str(page_index) +'.html')\n",
    "                break\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            is_err = False\n",
    "    if is_err:\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(res.text, \"html5lib\")\n",
    "    print(\"page\", page_index, end='\\r')\n",
    "    post_index = 0\n",
    "    \n",
    "    with open(csv_dir ,'a', encoding = 'utf-8-sig') as f_list:\n",
    "        w_list = csv.writer(f_list)\n",
    "        for entry in soup.select('.r-ent'):\n",
    "            post_index = post_index + 1\n",
    "            if post_index < start_post:\n",
    "                continue\n",
    "            else:\n",
    "                start_post = 0\n",
    "            entry_a = entry.find('a')\n",
    "            if not entry_a:\n",
    "                continue\n",
    "            if not entry_a['href']:\n",
    "                continue\n",
    "            \n",
    "            #### Link web and read content, title and push each post. ####\n",
    "            is_err = True\n",
    "            err_count = 0\n",
    "            po_web = ptt_web + entry_a['href']\n",
    "            rs = requests.session()\n",
    "            while is_err:\n",
    "                try:\n",
    "                    sub_res = rs.post(over18_web, data=payload18)\n",
    "                    sub_res = rs.get(po_web, verify=False)\n",
    "                    #print(po_web)\n",
    "                    #sub_res = requests.get(po_web)\n",
    "                except BaseException:\n",
    "                    err_count = err_count + 1\n",
    "                    if err_count > err_max:\n",
    "                        print('ERROR post: ', post_index, 'WEB: ', po_web)\n",
    "                        break\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    is_err = False\n",
    "            if is_err:\n",
    "                continue\n",
    "                \n",
    "            # Get all content for this post.\n",
    "            sub_soup = BeautifulSoup(sub_res.text, \"html5lib\")\n",
    "            main_content = sub_soup.select('#main-content')\n",
    "            if not main_content:\n",
    "                continue\n",
    "\n",
    "            po_list = main_content[0].select('.article-meta-value')\n",
    "\n",
    "            if not po_list:\n",
    "                continue\n",
    "\n",
    "            if len(po_list) < 4:\n",
    "                continue\n",
    "                \n",
    "            # Get date and time for this post.\n",
    "            po_datetime = po_list[3].text\n",
    "            if not po_datetime:\n",
    "                continue\n",
    "            if not len(po_datetime) == 24:\n",
    "                continue\n",
    "            po_week = po_datetime[0:3]\n",
    "            try:\n",
    "                po_time = time.mktime(time.strptime(po_datetime,\"%a %b %d %H:%M:%S %Y\"))\n",
    "            except BaseException:\n",
    "                continue\n",
    "                \n",
    "            # Get content type for this post.\n",
    "            po_name = po_list[0].text\n",
    "            if po_name:\n",
    "                index1 = po_name.find(' (')\n",
    "                if index1 <= 0:\n",
    "                    po_id = 'guest'\n",
    "                else:\n",
    "                    po_id = po_name[0:index1]\n",
    "            else:\n",
    "                po_id = 'guest'\n",
    "            \n",
    "            po_title = po_list[2].text\n",
    "            if po_title:\n",
    "                index1 = po_title.find('[')\n",
    "                index2 = po_title.find(']')\n",
    "                if index1 < 0 or index2 <= index1:\n",
    "                    po_type = '無題'\n",
    "                else:\n",
    "                    po_type = po_title[index1+1:index2]\n",
    "            else:\n",
    "                po_type = '無題'\n",
    "            \n",
    "            # Get content title for this post.\n",
    "            po_title = po_title[index2+1:]\n",
    "            po_pushnum = len(main_content[0].select('.push'))\n",
    "            \n",
    "            po_text = ''\n",
    "            content_cnt = 0\n",
    "            old_ptt_po = False\n",
    "            for add_text in main_content[0].strings:\n",
    "                content_cnt = content_cnt + 1\n",
    "                # Pass title information.\n",
    "                if content_cnt <= 8:\n",
    "                    continue\n",
    "                # Get the slitted point between content and pushs.\n",
    "                elif add_text[0:21] == '※ 發信站: 批踢踢實業坊(ptt.cc)':\n",
    "                    if add_text == '※ 發信站: 批踢踢實業坊(ptt.cc)\\n' or add_text == '※ 發信站: 批踢踢實業坊(ptt.cc) \\n':\n",
    "                        old_ptt_po = True\n",
    "                    else:\n",
    "                        po_ip = row_1st_keep(add_text)\n",
    "                        break\n",
    "                elif old_ptt_po:\n",
    "                    po_ip = row_1st_keep(add_text)\n",
    "                    break\n",
    "                else:\n",
    "                    po_text = po_text + add_text\n",
    "            # Read each push.\n",
    "            po_wordnum = len(po_text)\n",
    "            po_push = ''\n",
    "            for add_push in main_content[0].select('.push'):\n",
    "                for add_pushtext in add_push.strings:\n",
    "                    po_push = po_push + ',' + add_pushtext\n",
    "\n",
    "            postInfo = [(page_index - 1) * 20 + post_index, po_type, po_week, po_time, po_id, po_wordnum, po_pushnum, po_web, po_ip, po_title]\n",
    "            w_list.writerow(postInfo)\n",
    "            # Save all content for this post.\n",
    "            with open(content_dir+'/'+str((page_index - 1) * 20 + post_index)+'.csv' ,\n",
    "                      'a', encoding = 'utf-8-sig') as f_text:\n",
    "                w_text = csv.writer(f_text)\n",
    "                w_text.writerow([po_text])\n",
    "            # Save each push for this post.\n",
    "            with open(push_dir+'/'+str((page_index - 1) * 20 + post_index)+'.csv' ,\n",
    "                      'a', encoding = 'utf-8-sig') as f_text:\n",
    "                w_text = csv.writer(f_text)\n",
    "                w_text.writerow([po_push])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "vjFd0P8ZINpI",
    "outputId": "37b1cd1a-4731-413a-a541-9db6b9a00257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 2000 post 20 file 1520492468.9048126\n"
     ]
    }
   ],
   "source": [
    "if start_post < post_index:\n",
    "    start_post = post_index\n",
    "print(\"page\",page_index ,\"post\" ,start_post ,\"file\", now_time)\n",
    "f_list.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "PTT_crawler.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
